import numpy as np

def ql_policy(x_d_int: int,
              episodes: int = 50_000,
              max_steps: int = 10_000,
              alpha: float = 0.5,
              eps_start: float = 0.2,
              eps_end: float = 0.01,
              eps_decay: float = 0.9995,
              seed: int | None = None):
    S, A = 2**n, len(ALL_ACTIONS)
    rng = np.random.default_rng(seed)
    Q = np.zeros((S, A), dtype=float)

    def start_state():
        s = rng.integers(0, S - 1)
        return s if s < x_d_int else s + 1

    eps = eps_start
    for _ in range(episodes):
        s = start_state()
        for _ in range(max_steps):
            a = int(rng.integers(0, A)) if rng.random() < eps else int(np.argmax(Q[s]))
            sp = sample_next_state(s, ALL_ACTIONS[a])
            target = gamma * (1.0 if sp == x_d_int else float(np.max(Q[sp])))
            if s != x_d_int:
                Q[s, a] += alpha * (target - Q[s, a])
            if sp == x_d_int:
                break
            s = sp
        eps = max(eps_end, eps * eps_decay)

    pi_idx = np.argmax(Q, axis=1).astype(int)
    pi_idx[x_d_int] = 0
    pi_u = [ALL_ACTIONS[i] for i in pi_idx]
    return pi_idx, pi_u, Q
